# Saved Models:

This directory holds many types of models generated by the notebook. The findings are as follows:

* Vanilla model is fairly big. The size can be reduced by using quantization aware training (which makes compression 
  operation easier when saving the model.) Quantization can be done in post training as well.
  Quantization aware training works better; however it is not suitable and tools for it is not 
  readily available (if not used correctly, it is far less efficient).
    
* Knowledge distillation can be used to shrink the number of parameters which
increases the inference speed significantly and reducing the model size.
  However, the learner (low capacity network) has to be engineer precisely to 
  work well with the initial teacher (high capacity network). This method is not destructive as other methods.
  
* I was able to compress the model from 18 MB down to 1.56 MB (best: pruning) using methods mentioned above.
Inference time was the best with knowledge distillation as learner is a smaller network; that has less number of parameters.
  The cost of knowledge distillation is in accuracy; the model accuracy dropped to 80% from 90%. Inference time reduced from
  1.7 seconds to 0.8 seconds.