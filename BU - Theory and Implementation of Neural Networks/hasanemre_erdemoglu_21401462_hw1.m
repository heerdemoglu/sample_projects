% *************************************************************************
% EEE 443 - Neural Networks - Assignment 1
% Hasan Emre Erdemoglu - 21401462
% *************************************************************************
function hasanemre_erdemoglu_21401462_hw1(question)
clc
close all

switch question
	case '1'
		disp('Question 1:')
		% This question normally does not have a MATLAB counterpart.
		question1()
	case '2'
		disp('Question 2:')
		% question 2 code goes here
		question2()
		
	case '3'
		disp('Question 3:')
		% question 3 code goes here
		question3()
end
end

%% Question Functions:
function question1()
disp('No output is available.')
end
function question2()
% Clear everyhing:
% *************************************************************************
% Noise is N(0,0.2). Truth table for quadruple (x1, x2, x3, x4) is
% generated by hand. It is expanded with bias term -1, therefore we have
% 16x5 matrix.
%
% Then we generate the logic table for the neurons. Then we expand the
% logic table to have more samples and we add Gaussian noise.
%
% The ground truths for the XOR operation was calculated by hand. It is a
% very fundamental calculation which involves filling up the SOP logic
% table. Since this operation is fairly easy it is omitted from the
% report and the final results are hard coded here.
%
% The rest of the code just uses hidden layer bias/weights and output layer
% weights and biases to construct the neural realization of given logic
% circuit.
%
% Note that, as the question focuses on Hidden Layer and robustness through
% hidden layer, I omitted the robustness of output layer. Hidden layer
% realizes AND operations using triplets of inputs (with possible inverses)
% where in marginally stable case sum of weights + bias should be equal to
% zero at least. By doing that, bias enforces all inputs to be correct to
% fire the neuron. Output layer realizes OR gate and bias must be equal to
% weight of one of the neurons. (Since its a OR operation on binary inputs
% all weights can be equalized and normalized). However, this makes the
% neural network fragile to any kind of noise.
%
% Due to this reason, I assume if the hidden layer is fully robust,
% marginal case of [1 1 1 1 -1] will work just fine in the output layer.
% However, even when best support vectors are selected, the noise may
% perturb the outputs of the hidden layer. Even when we select the best
% support vectors for the output layer too, noise may still overcome the
% signal, if large enough. Therefore it is not possible to guarantee,
% perfect performance on the neural network, given noisy data.
%
% Summary for output robustness: The question didnt ask about it and so I
% assumed that I am not asked to implement robustness there.
%
% Self note: Normalization of weights could be nice,smaller coefficients
% imply smaller error terms.
% *************************************************************************

% Part D:
mu = 0; sigma = 0.2; % Gaussian noise parameters

% Generate input samples:

% First create noise-free logic table: [X|-1]
x = [0 0 0 0 -1; 0 0 0 1 -1; 0 0 1 0 -1; 0 0 1 1 -1; ...
	0 1 0 0 -1; 0 1 0 1 -1; 0 1 1 0 -1; 0 1 1 1 -1; ...
	1 0 0 0 -1; 1 0 0 1 -1; 1 0 1 0 -1; 1 0 1 1 -1; ...
	1 1 0 0 -1; 1 1 0 1 -1; 1 1 1 0 -1; 1 1 1 1 -1];

% From input samples, generate neuron based logic tables:
x_in = zeros(4,5,16); % pre-init for fast computation
for i=1:size(x,1)
	% Realizes neuron 1's input from logic table, x:
	x_in(1,:,i) = [x(i,1), x(i,2), x(i,3), x(i,4), -1];
	% Realizes neuron 2's input from logic table, x:
	x_in(2,:,i) = [x(i,1), not(x(i,2)), x(i,3), x(i,4), -1];
	% Realizes neuron 3's input from logic table, x:
	x_in(3,:,i) = [not(x(i,1)), x(i,2), not(x(i,3)), x(i,4), -1];
	% Realizes neuron 4's input from logic table, x:
	x_in(4,:,i) = [not(x(i,1)), x(i,2), x(i,3), not(x(i,4)), -1];
	
end

% Generate output from XOR operation: A*notB + notA*B
% A = X1 + notX2;    B = notX3 + notX4; out = A*notB + notA*B
% out = (X1 + notX2) * not(notX3 + notX4)+not(X1 + notX2) * (notX3 + notX4)
y = zeros(16,1); y(4:7) = 1; y(12) = 1; y(16) = 1;

% Now repeat this 25 times:
xx = x_in; % debug
x_in = repmat(x_in, [1, 1, 25]);
y = repmat(y, [25, 1]);

% Now generate Gaussian noise vector for the inputs - 400x4 noise terms
noise = normrnd(mu, sigma, [4,5,400]); % only data samples are affected by noise.
noise(:,5,:) = 0; % exclude bias from noise

% Add noise to the input data:
x_in_noise = x_in + noise;

% Form the neural networks: Hidden layer weights taken from Part B and C:
% Due to implementing the logic inversions above, negative signs are
% omitted.
wh_b = [2,  0, 1.5, 2, 5; ...
	0, 3, 3,   4, 9; ...
	2,  8, 3,   0, 12; ...
	2,  8, 0,  3, 12];

% 3 neurons in each row: No noise --> 1+1+1-3=0 is marginally stable and
% correct. We have mean 0, std 0.2: AND operation multiplies.
% Multiplication of 3 Gaussians
wh_c = [1,  0, 1, 1, 2.717; ...
	0, 1, 1,   1, 2.717; ...
	1,  1, 1,   0, 2.717; ...
	1,  1, 0,  1, 2.717];

w_o = [1 1 1 1 -1]; % Weights for the output layer. Explained in report.

% Part b: - My weights:
part_b_output = realizeMyNetwork(x_in,wh_b,w_o);
part_b_noise_output = realizeMyNetwork(x_in_noise,wh_b,w_o);

perct_corr_b = perc_correct(y,part_b_output, size(y,1));
perct_corr_b_noise = perc_correct(y,part_b_noise_output, size(y,1));

% Display results:
disp('Percentage Correct for Part B Weights with no noise present:')
disp(perct_corr_b);
disp('Percentage Correct for Part B Weights with noise present:')
disp(perct_corr_b_noise);
disp('Therefore given weights on part B are not quite robust to noise.')

% Part c:
part_c_output = realizeMyNetwork(x_in,wh_c,w_o);
part_c_noise_output = realizeMyNetwork(x_in_noise,wh_c,w_o);

perct_corr_c = perc_correct(y,part_c_output, size(y,1));
perct_corr_c_noise = perc_correct(y,part_c_noise_output, size(y,1));

% Display results:
disp('*******************************************************************')
disp('Percentage Correct for Part C Weights with no noise present:')
disp(perct_corr_c);
disp('Percentage Correct for Part C Weights with noise present:')
disp(perct_corr_c_noise);
disp('The performance improved significantly, this is more robust.')
disp('Please check the report to see why this is the most robust weights.')
end
function question3()
% Load the dataset
load assign1_data1.mat

% Normalize images:
trainims = double(trainims) / 255;
testims = double(testims) / 255;

% Dataset Explanation: ****************************************************
% Training: 28x28 images - 5200 samples
% Test: 28x28 images - 1300 samples
%
% Images are kept in unsigned 8 bit integers.
% Color is grayscale, scaled between 0 and 255.
%
% Labels for the sets are kept seperately.
% Image and label indices match for both sets.
%
% There are 26 classes available in the dataset, sorted, groupped &
% ordered from 1 to 26.
%
% The images contain English letters, 1 being 'A' and 26 being 'Z'.
% These are deduced using observation on data.
% *************************************************************************

%% Part A:
% We have 26 classes, 28x28 pixels shown on a 6x5 matrix.
classes = 26; pxls = 28;  x_ax = 5;  y_ax = 6;

% Visualize sample image from each class:
% Since no specification given, I will pick first index of the given
% class, from training images.

% Sample image pre-allocation:
smpl_ims = zeros(pxls, pxls, classes);
for i = 1:classes
	find_im_idx = find(trainlbls==i,1);
	smpl_ims(:,:,i) = trainims(:,:,find_im_idx);
	smpl_im = trainims(:,:,find_im_idx);
	
	subplot(x_ax,y_ax,i); imshow(smpl_im);
	colormap(gray);
end

% Beautify plot by adding title: -- Works after 2018b, comment if fails.
sgtitle('Alphabet Letters Labeled From 1-26 (Left to Right, Up to Down)');

% Correlation Coefficients Calculation:
corr_vals = zeros(classes,classes);
for i = 1:classes
	for j=1:classes
		corr_vals(i,j) = corr2(smpl_ims(:,:,i),smpl_ims(:,:,j));
	end
end

% Print Correlation values in matrix format:sgtitle('Alphabet Letters Labeled From 1-26 (Left to Right, Up to Down)');
% disp('Correlation Values in Matrix Form');
% disp(corr_vals); % display correlation in matrix form
figure; imagesc(corr_vals); colormap(gray); colorbar; %display image form
title('Correlation Matrix of Sample Letters')
% clc; close all; % ******************************************************* debug

%% Part B:
mu = 0; sigma = 0.01; % Given Gaussian variables.
its = 10000; % Given iteration rate
rng default; % for reproducability in MATLAB, adviced by documentation.

% Initialize w & b: NN total number of parameters 28x28
% - flattened for my ease, I will work with vectors rather than matrices
% note that we actually have [w|b] with given custom function.
% Rows: Connecting neuron index, Columns: Weights to next neuron
[w,b] = init_with_gauss(mu, sigma, classes, ...
	pxls, pxls);

% One hot-encode training and test data labels: To be used later:
% Rows: Labels, Columns: Neuron index for the label
trainlbls_onehot = encode_onehot(trainlbls);
testlbls_onehot = encode_onehot(testlbls);

% Flatten the input images:
% Columns: Image samples, Rows: Data for given column (image sample)
trainims = double(reshape(trainims, [size(trainims,1)*size(trainims,2) ...
	, size(trainims,3)]));
% trainims = [trainims; -ones(1,5200)];

testims = double(reshape(testims, [size(testims,1)*size(testims,2) ...
	, size(testims,3)]));
% testims = [testims; -ones(1,1300)];

% Learning & Tuning for Learning Rate:
% Online learning will be used. (update every sample)
% Keep the record in each iteration in memory, specifically, w, b and MSE
% values.
lr = 0:0.05:2;
for l = 1:size(lr,2)
	[w_lr(:,:,l),b_lr(:,l),mse_lr(:,l)] = ...
		learn_weights(trainims,trainlbls_onehot,w, b, lr(l), its);
end

% Pick best learning rate: -- take minimizinng MSE over iterations
[minMSE, best_lr_idx] = min(sum(mse_lr));

% Display results:
disp('Best Learning Rate Found:'); lr_star = lr(best_lr_idx);
disp(lr_star);
%%
figure();
plot(lr,sum(mse_lr)/10000);
title('MSE vs Learning Rate'); xlabel('Learning Rate'); ylabel('MSE');
hold on; plot(lr(best_lr_idx),(mse_lr(best_lr_idx))/10,'*r')
legend('lr vs MSE','Best point')

%% Show network weights for the best learning rate available:
figure;
for i = 1:26
	best_w(i,:,:) = reshape(w_lr(i,:,best_lr_idx), [28, 28]);
	subplot(x_ax,y_ax,i);
	imagesc(squeeze(best_w(i,:,:))); colormap(gray);
	% 	imshow(squeeze(best_w(i,:,:))*10); %scaled for better visual
end
sgtitle('Finalized Weights of Neuron from 1 to 26');

%% Part C:
lr_low = 1/100 * lr_star;
lr_high = 100 * lr_star;

lr_c = [lr_low, lr_star, lr_high];

% Do the training for given set of learning rates
for k = 1:3
	[w_lrc(:,:,k),b_lrc(:,k),mse_lrc(:,k)] = ...
		learn_weights(trainims,trainlbls_onehot,w, b, lr_c(k), its);
end

%% Plot MSE curves:
figure(); xlabel('Iterations'); ylabel('MSE values'); hold on;
plot(1:its,mse_lrc(:,1)); plot(1:its,mse_lrc(:,2));
plot(1:its,mse_lrc(:,3));
legend('lr-low', 'lr-best', 'lr-high'); grid on;
title('MSE evolution over iterations')
%%
% Display total average mean squared error:
averages = 1/10000 * sum(mse_lrc);
disp('Average MSE of lr_low:');
disp(averages(1));

disp('Average MSE of lr_best:');
disp(averages(2));

disp('Average MSE of lr_high:');
disp(averages(3));

%% Part D:
% Do the training for given set of learning rates.
% Using the weights from part C: 'w_lrc'
w_dlow = w_lrc(:,:,1); w_dstar = w_lrc(:,:,2); w_dhigh = w_lrc(:,:,3);
b_dlow = b_lrc(:,1); b_dstar = b_lrc(:,2); b_dhigh = b_lrc(:,3);

% Calculate the output given the input
out_dlow = sigmoid((w_dlow*testims)-repmat(b_dlow, [1,1300]));
out_dstar = sigmoid((w_dstar*testims)-repmat(b_dstar, [1,1300]));
out_dhigh = sigmoid((w_dhigh*testims)-repmat(b_dhigh, [1,1300]));

% Winner takes all: May the best neuron win: Get winning idx per each
% sample.
[vals_dlow, indices_dlow] = max(out_dlow);
[vals_dstar, indices_dstar] = max(out_dstar);
[vals_dhigh, indices_dhigh] = max(out_dhigh);

% Now do check classification error using original labels, one hot enode
% not needed as comparison from indices are easy:
perc_corr_dlow = (1 - sum(abs(indices_dlow ~= testlbls')) / 1300);
perc_corr_dstar = (1 - sum(abs(indices_dstar ~= testlbls')) / 1300);
perc_corr_dhigh = (1 - sum(abs(indices_dhigh ~= testlbls')) / 1300);

% Display the results:
disp('Percentage of output correct for lr: 0.0005 - Low Learning Rate');
disp(perc_corr_dlow)
disp('*************')
disp('Percentage of output correct for lr: 0.05 - Best Learning Rate');
disp(perc_corr_dstar)
disp('*************')
disp('Percentage of output correct for lr: 5 - High Learning Rate');
disp(perc_corr_dhigh)
end
%% Question Helper Functions:
% Question 2 Helpers:
function out = not(in)
% Does boolean inversion for the inputs.
if in == 1
	out = 0;
else
	out = 1;
end
end
function [actv_sig_out_store] = realizeMyNetwork(x, w_hidden, w_out)
sampSize = size(x,3);
for i = 1:sampSize
	% Calculate activations for hidden layer:
	actv_sig_hl = diag(w_hidden*x(:,:,i)'); % diagonal entries for weighted sums
	
	% Step function applied to each neuron.
	actv_sig_hl(actv_sig_hl >= 0) = 1;
	actv_sig_hl(actv_sig_hl < 0) = 0;
	
	actv_sig_hl = [actv_sig_hl; 1]; % Inp to Out layer, bias ext'd.
	
	% Calculate activations for the output layer:
	actv_sig_out = w_out*actv_sig_hl;
	
	% Step function applied to output neuron:
	actv_sig_out(actv_sig_out >= 0) = 1;
	actv_sig_out(actv_sig_out < 0) = 0;
	
	% Store elements:
	actv_sig_out_store(i,1) = actv_sig_out;
	
end
end
function correct = perc_correct(y_act, y_pred, sample_size)
% Calculate percentage correct answers.
error = abs(y_act - y_pred); % make -1 and +1 errors the same

no_errors = sum(error);

correct = (sample_size - no_errors)/ sample_size;
end
% Question 3 Helpers:
function out = sigmoid(z)
% Calculates sigmoid function given a matrix value set.
% Used to simplify code in Question 3 - Part B.
out = 1./(1+exp(-z));
end
function [w,b] = init_with_gauss(mu, sigma, layerSize, imSize_x, imSize_y)
% Initialize weights and bias by gaussian distribution, given layer size
% Customized to convert 2D image connections, flattening them to vectors
w = normrnd(mu, sigma, [layerSize,imSize_x*imSize_y]); % includes bias term
b = normrnd(mu, sigma, [layerSize, 1]);
end
function [encoded] = encode_onehot(labels)
% Encode 1-26 labels to one hot version.
encoded = zeros(size(labels,1),26);
for i=1:size(labels,1)
	encoded(i,labels(i)) = 1;
end
end
function [w, b, mse] = ...
	learn_weights(images_flat,labels_onehot,wb_init,b_init,lr, iterations)
% This function does the learning of the weights, bias and MSE and also
% updates weights. Specifically written to ease read in Parts B,C,D.

w = wb_init; b = b_init; mse = zeros(1,iterations);
for it = 1:iterations
	% Pick a random sample: - Already normalized. Flattened to ease my code
	% Image converted to double to allow matrix calculations with w.
	idx = ceil(size(images_flat,2)*rand);
	x = images_flat(:,idx); % flattened img selected, with bias added
	
	% Do feedforward operation with sigmoid:
	output = sigmoid(w*x-b)';
	error = (labels_onehot(idx,:)-output);
	
	% MSE loss at this stage
	mse(it) = 1/26 .* error * error';
	
	% Do weight update: - Gradient Descent w/ sigmoid:
	dw = -2*(error .* ((output) .* (1-(output)))) .* x;
	db = -2*(error .* ((output) .* (1-(output))));
	
	% Optimizes the parameters, -- Namely, update w and b
	w = w - lr*dw';
	b = b - lr*db';
	
	% Returns MSE for 10000 iterations and final w and b
end
end