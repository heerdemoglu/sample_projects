{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 8: Web Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The aim of this lab is for students to get experience with **Web Mining** methods covered in week 9.\n",
    "\n",
    "- This lab is the first part of a **two-week assignment** that covers weeks 9 and 10.\n",
    "- This lab corresponds to **Assignment 4** which is due on **Tuesday 8th December at 10am**, accounting for 10% of your overall grade. Questions in this lab sheet will contribute to 5% of your overall grade; questions in the lab sheet for week 10 will cover for another 5% of your overall grade.\n",
    "- <font color = 'maroon'>The last section of this notebook includes the questions that are assessed towards your final grade.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes about the assignment: \n",
    "\n",
    "- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n",
    "- The total assessed coursework is worth 40% of your final grade.\n",
    "- There will be 9 lab sessions and 4 assignments.\n",
    "- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n",
    "- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n",
    "- You are asked to submit a **report** that should answer the questions specified in the last section of this notebook. The report should be in **PDF format** (so **NOT** *doc, docx, notebook* etc). It should be well identified with your name, student number, assignment number (for instance, Assignment 4), module, and marked with question numbers. \n",
    "- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n",
    "- Please name your report as follows: Assignment4-StudentName-StudentNumber.pdf\n",
    "- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping using Python\n",
    "\n",
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. In this lab notebook, we will be working on data extraction from the web using Python's [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) module. Please make sure to familiarise yourselves with the [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), or to refer to the documentation if you need more information for a particular class or function.\n",
    "\n",
    "The dataset to be used in the first 3 sections of the notebook is taken from a 10km race that took place in Hillsboro, USA on June 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opening HTML content\n",
    "\n",
    "Most websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. HTML elements are separated by tags and they directly introduce content to the web page. Here is how a basic HTML document looks like: [https://www.w3schools.com/html/html_basic.asp](https://www.w3schools.com/html/html_basic.asp) - please take some time to study the link and the HTML code, we will come back into that later during the tutorial.\n",
    "\n",
    "We can see that the content of the first heading is contained between the ‘h1’ tags. The first paragraph is contained between the ‘p’ tags. On a real website, we need to find out between which tags the relevant data is and tell it to our scraper. We also need to specify which links should be explored and where they can be found among the HTML file. With all this information, our scraper should be able to gather the required data.\n",
    "\n",
    "We first start by loading standard python modules for data mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform web scraping, we also import the libraries shown below. The urllib.request module is used to open URLs. The Beautiful Soup package is used to extract data from HTML files. The Beautiful Soup library's name is bs4 which stands for Beautiful Soup, version 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary modules, we specify the URL containing the dataset mentioned above and pass it to urlopen() to get the HTML contents of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.hubertiming.com/results/2017GPTR\"\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing HTML content \n",
    "\n",
    "Opening the HTML content of the page is just the first step. The next step is to create a Beautiful Soup object from the HTML content. This is done by passing the html content to the BeautifulSoup() function. The Beautiful Soup package is used to parse the HTML content, that is, take the raw HTML text and break it into Python objects. The second argument 'lxml' is the HTML parser whose details we do not need to worry about at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soup object allows you to extract interesting information about the website we are scraping such as getting the title of the page as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Race results for the 2017 Intel Great Place to Run \\ Urban Clash Games!</title>\n"
     ]
    }
   ],
   "source": [
    "# Get the title\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the text of the webpage and quickly print it out to check if it is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the text\n",
    "text = soup.get_text()\n",
    "#print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the html content of the [webpage we are scraping](https://www.hubertiming.com/results/2017GPTR) by opening the webpage in another tab in a web browser, right-clicking anywhere on the webpage and selecting \"View Source\" or \"View Page Source\" (for Chrome and Firefox respectively - similar options to view the HTML source exist for other web browsers). \n",
    "\n",
    "Please take some time to inspect the html content of the webpage and spot for examples of useful tags. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells.\n",
    "\n",
    "We can use the find_all() method of soup to extract useful html tags within a webpage. The code below shows how to extract all the **hyperlinks** within the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"mailto:timing@hubertiming.com\">timing@hubertiming.com</a>,\n",
       " <a href=\"https://www.hubertiming.com/\">Huber Timing Home</a>,\n",
       " <a class=\"btn btn-primary btn-lg\" href=\"/results/2017GPTR10K\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i aria-hidden=\"true\" class=\"fa fa-user\"></i> 10K</a>,\n",
       " <a class=\"btn btn-primary btn-lg\" href=\"/results/summary/2017GPTR\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i class=\"fa fa-stream\"></i> Summary</a>,\n",
       " <a id=\"individual\" name=\"individual\"></a>,\n",
       " <a data-url=\"/results/2017GPTR\" href=\"#tabs-1\" id=\"rootTab\" style=\"font-size: 18px\">5K Results</a>,\n",
       " <a href=\"https://www.hubertiming.com/\"><img height=\"65\" src=\"https://www.hubertiming.com//sites/all/themes/hubertiming/images/clockWithFinishSign_small.png\" width=\"50\"/>Huber Timing</a>,\n",
       " <a href=\"https://facebook.com/hubertiming/\"><img src=\"https://www.hubertiming.com/results/FB-f-Logo__blue_50.png\"/></a>,\n",
       " <a class=\"small\" id=\"bestFeatureEver\" style=\"color:#007bff\">Dark Mode</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, HTML tags sometimes come with attributes such as *class* and *src*. These attributes provide additional information about html elements. We can use a for loop and the get(\"href\") method to extract and print out only hyperlinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mailto:timing@hubertiming.com\n",
      "https://www.hubertiming.com/\n",
      "/results/2017GPTR10K\n",
      "/results/summary/2017GPTR\n",
      "None\n",
      "#tabs-1\n",
      "https://www.hubertiming.com/\n",
      "https://facebook.com/hubertiming/\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out table rows only, pass the 'tr' argument in soup.find_all():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 table rows\n",
    "rows = soup.find_all('tr')  # the 'tr' tag in html denotes a table row\n",
    "#print(rows[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting an HTML table into a Pandas dataframe\n",
    "\n",
    "The goal of this lab notebook is to take a table from a webpage and convert it into a pandas dataframe for easier manipulation using Python. For an example on how are tables encoded in HTML please study the following [example table](https://www.w3schools.com/tags/tag_tr.asp). As you'll see from the above example table, rows in HTML tables are identified using the 'tr' tag; each HTML table has a header identified by the 'th' tag; and each cell in the table is identified by the 'td' tag. Please take some time to familiarise yourselves with the example table html code in the above link.\n",
    "\n",
    "\n",
    "To convert the HTML table to a pandas dataframe, we should get all table rows in list form first and then convert that list into a dataframe. Below is a for loop that iterates through table rows and prints out the cells of the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in rows:\n",
    "    row_td = row.find_all('td')  # the 'td' tag in html code denotes a table cell\n",
    "    #print(row_td)\n",
    "type(row_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that each row is printed with html tags embedded in each row. This is not what we want. We can remove the html tags using Beautiful Soup or regular expressions. Using regular expressions is highly discouraged since it requires several lines of code and one can easily make mistakes. It requires importing the *re* (for regular expressions) module.\n",
    "\n",
    "The easiest way to remove html tags is to use Beautiful Soup, and it takes just one line of code to do this. We pass the string of interest into BeautifulSoup() and use the get_text() method to extract the text without html tags. The following is an example of removing html tags for one row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1458, 1400, \n",
      "\n",
      "                    SUMALATHA PURMA\n",
      "\n",
      "                , F, PORTLAND, OR, 1:48:13, 34:54, 1:48:13]\n"
     ]
    }
   ],
   "source": [
    "str_cells = str(row_td)\n",
    "cleantext = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "print(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having removed HTML tags for one row, we can now convert the entire HTML table into a pandas dataframe.\n",
    "\n",
    "First, we try to scrape the header of the table, which includes names for all the table attributes. We create an empty list object, and using Beautiful Soup we locate the 'th' HTML tag which denotes a table header. We convert the header from HTML to a string, and then append it to the list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Place, Bib, Name, Gender, City, State, Chip Time, Chip Pace, Gun Time]']\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list where the table header will be stored\n",
    "header_list = []\n",
    "\n",
    "# Find the 'th' html tags which denote table header\n",
    "col_labels = soup.find_all('th')\n",
    "col_str = str(col_labels)\n",
    "cleantext_header = BeautifulSoup(col_str, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "header_list.append(cleantext_header) # Add the clean table header to the list\n",
    "\n",
    "print(header_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the header above contains 9 elements, separated by commas. \n",
    "\n",
    "Now, we do the same process as above but for every row in the table that contains cell elements identified by the 'td' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list where the table will be stored\n",
    "table_list = []\n",
    "\n",
    "# For every row in the table, find each cell element and add it to the list\n",
    "for row in rows:\n",
    "    row_td = row.find_all('td')\n",
    "    row_cells = str(row_td)\n",
    "    row_cleantext = BeautifulSoup(row_cells, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "    table_list.append(row_cleantext)  # Add the clean table row to the list\n",
    "    \n",
    "#print(table_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the table_list list object includes all information stored in the original table, where elements in each row are separated by commas. We also see a lot of special and uneccessary characters that would need to be removed later on.\n",
    "\n",
    "Now, we have a python list object for the header called 'header_list' and another list object for the main table called 'table_list'. We can now convert the header list into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Place, Bib, Name, Gender, City, State, Chip T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [Place, Bib, Name, Gender, City, State, Chip T..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header = pd.DataFrame(header_list)\n",
    "df_header.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is not in the format we want, since it only includes one column instead of 9 columns. To clean it up, we should split the \"0\" column into multiple columns at the comma position. This is accomplished by using the str.split() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Place</td>\n",
       "      <td>Bib</td>\n",
       "      <td>Name</td>\n",
       "      <td>Gender</td>\n",
       "      <td>City</td>\n",
       "      <td>State</td>\n",
       "      <td>Chip Time</td>\n",
       "      <td>Chip Pace</td>\n",
       "      <td>Gun Time]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1      2        3      4       5           6           7  \\\n",
       "0  [Place   Bib   Name   Gender   City   State   Chip Time   Chip Pace   \n",
       "\n",
       "            8  \n",
       "0   Gun Time]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header2 = df_header[0].str.split(',', expand=True)\n",
    "df_header2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can carry out the same process as above to convert the table list into a pandas dataframe for the table values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Finishers:</td>\n",
       "      <td>1458]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Male:</td>\n",
       "      <td>771]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Female:</td>\n",
       "      <td>687]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1</td>\n",
       "      <td>2320</td>\n",
       "      <td>\\r\\n\\r\\n                    DANIEL M HINCKLEY...</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>16:42</td>\n",
       "      <td>5:23</td>\n",
       "      <td>16:44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[2</td>\n",
       "      <td>2335</td>\n",
       "      <td>\\r\\n\\r\\n                    KORY F GRAY\\r\\n\\r...</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>17:34</td>\n",
       "      <td>5:40</td>\n",
       "      <td>17:35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[3</td>\n",
       "      <td>1770</td>\n",
       "      <td>\\r\\n\\r\\n                    FILIP SCHMOLE\\r\\n...</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:13</td>\n",
       "      <td>5:52</td>\n",
       "      <td>18:14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[4</td>\n",
       "      <td>2584</td>\n",
       "      <td>\\r\\n\\r\\n                    TRENTON C ROLLING...</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:32</td>\n",
       "      <td>5:58</td>\n",
       "      <td>18:35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[5</td>\n",
       "      <td>2688</td>\n",
       "      <td>\\r\\n\\r\\n                    YEAN-AN LIAO\\r\\n\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:12</td>\n",
       "      <td>6:11</td>\n",
       "      <td>19:18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1                                                  2  \\\n",
       "0           []    None                                               None   \n",
       "1  [Finishers:   1458]                                               None   \n",
       "2       [Male:    771]                                               None   \n",
       "3     [Female:    687]                                               None   \n",
       "4           []    None                                               None   \n",
       "5           [1    2320   \\r\\n\\r\\n                    DANIEL M HINCKLEY...   \n",
       "6           [2    2335   \\r\\n\\r\\n                    KORY F GRAY\\r\\n\\r...   \n",
       "7           [3    1770   \\r\\n\\r\\n                    FILIP SCHMOLE\\r\\n...   \n",
       "8           [4    2584   \\r\\n\\r\\n                    TRENTON C ROLLING...   \n",
       "9           [5    2688   \\r\\n\\r\\n                    YEAN-AN LIAO\\r\\n\\...   \n",
       "\n",
       "      3           4     5       6      7        8  \n",
       "0  None        None  None    None   None     None  \n",
       "1  None        None  None    None   None     None  \n",
       "2  None        None  None    None   None     None  \n",
       "3  None        None  None    None   None     None  \n",
       "4  None        None  None    None   None     None  \n",
       "5     M   HILLSBORO    OR   16:42   5:23   16:44]  \n",
       "6     M   HILLSBORO    OR   17:34   5:40   17:35]  \n",
       "7     M    PORTLAND    OR   18:13   5:52   18:14]  \n",
       "8     M    PORTLAND    OR   18:32   5:58   18:35]  \n",
       "9     M   HILLSBORO    OR   19:12   6:11   19:18]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_table = pd.DataFrame(table_list)\n",
    "df_table2 = df_table[0].str.split(',', expand=True)\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better, but there is still work to do. The dataframe has unwanted square brackets surrounding each row. It also has some line and carriage return characters that can be removed (\\r, \\n). We can use the strip() method to remove the square brackets and uneccesary characters on columns 0, 1, 2 and 8. \n",
    "\n",
    "We also notice that the first few rows of thable contain overall statistics on the race, and are not formatted as the rest of the table rows, containing missing values. Therefore any rows with missing values can be removed from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2320</td>\n",
       "      <td>DANIEL M HINCKLEY</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>16:42</td>\n",
       "      <td>5:23</td>\n",
       "      <td>16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2335</td>\n",
       "      <td>KORY F GRAY</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>17:34</td>\n",
       "      <td>5:40</td>\n",
       "      <td>17:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1770</td>\n",
       "      <td>FILIP SCHMOLE</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:13</td>\n",
       "      <td>5:52</td>\n",
       "      <td>18:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2584</td>\n",
       "      <td>TRENTON C ROLLING</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:32</td>\n",
       "      <td>5:58</td>\n",
       "      <td>18:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2688</td>\n",
       "      <td>YEAN-AN LIAO</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:12</td>\n",
       "      <td>6:11</td>\n",
       "      <td>19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>1576</td>\n",
       "      <td>JORGE1 LOPEZ</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:19</td>\n",
       "      <td>6:14</td>\n",
       "      <td>19:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>1479</td>\n",
       "      <td>SCOTT E HAMPSHIRE</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:27</td>\n",
       "      <td>6:16</td>\n",
       "      <td>19:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>895</td>\n",
       "      <td>KEVIN CANADA</td>\n",
       "      <td>M</td>\n",
       "      <td>BEAVERTON</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:53</td>\n",
       "      <td>6:24</td>\n",
       "      <td>20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>2631</td>\n",
       "      <td>SCOTT GERWIG</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:57</td>\n",
       "      <td>6:26</td>\n",
       "      <td>19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>2431</td>\n",
       "      <td>NICOLAUS L ROCK</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>20:00</td>\n",
       "      <td>6:27</td>\n",
       "      <td>20:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1                  2   3           4    5       6      7       8\n",
       "5    1   2320  DANIEL M HINCKLEY   M   HILLSBORO   OR   16:42   5:23   16:44\n",
       "6    2   2335        KORY F GRAY   M   HILLSBORO   OR   17:34   5:40   17:35\n",
       "7    3   1770      FILIP SCHMOLE   M    PORTLAND   OR   18:13   5:52   18:14\n",
       "8    4   2584  TRENTON C ROLLING   M    PORTLAND   OR   18:32   5:58   18:35\n",
       "9    5   2688       YEAN-AN LIAO   M   HILLSBORO   OR   19:12   6:11   19:18\n",
       "10   6   1576       JORGE1 LOPEZ   M    PORTLAND   OR   19:19   6:14   19:20\n",
       "11   7   1479  SCOTT E HAMPSHIRE   M   HILLSBORO   OR   19:27   6:16   19:29\n",
       "12   8    895       KEVIN CANADA   M   BEAVERTON   OR   19:53   6:24   20:02\n",
       "13   9   2631       SCOTT GERWIG   M    PORTLAND   OR   19:57   6:26   19:59\n",
       "14  10   2431    NICOLAUS L ROCK   M   HILLSBORO   OR   20:00   6:27   20:01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove uneccesary characters\n",
    "df_table2[0] = df_table2[0].str.strip('[')\n",
    "df_table2[0] = df_table2[0].str.strip(']')\n",
    "df_table2[1] = df_table2[1].str.strip(']')\n",
    "df_table2[8] = df_table2[8].str.strip(']')\n",
    "df_table2[2] = df_table2[2].str.strip('\\r\\n\\r\\n ')\n",
    "\n",
    "# Remove all rows with any missing values\n",
    "df_table3 = df_table2.dropna(axis=0, how='any')\n",
    "\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Now we can concatenate the header dataframe with the table dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Place</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Chip Time</th>\n",
       "      <th>Chip Pace</th>\n",
       "      <th>Gun Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2320</td>\n",
       "      <td>DANIEL M HINCKLEY</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>16:42</td>\n",
       "      <td>5:23</td>\n",
       "      <td>16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2335</td>\n",
       "      <td>KORY F GRAY</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>17:34</td>\n",
       "      <td>5:40</td>\n",
       "      <td>17:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1770</td>\n",
       "      <td>FILIP SCHMOLE</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:13</td>\n",
       "      <td>5:52</td>\n",
       "      <td>18:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2584</td>\n",
       "      <td>TRENTON C ROLLING</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>18:32</td>\n",
       "      <td>5:58</td>\n",
       "      <td>18:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2688</td>\n",
       "      <td>YEAN-AN LIAO</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:12</td>\n",
       "      <td>6:11</td>\n",
       "      <td>19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>1576</td>\n",
       "      <td>JORGE1 LOPEZ</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:19</td>\n",
       "      <td>6:14</td>\n",
       "      <td>19:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>1479</td>\n",
       "      <td>SCOTT E HAMPSHIRE</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:27</td>\n",
       "      <td>6:16</td>\n",
       "      <td>19:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>895</td>\n",
       "      <td>KEVIN CANADA</td>\n",
       "      <td>M</td>\n",
       "      <td>BEAVERTON</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:53</td>\n",
       "      <td>6:24</td>\n",
       "      <td>20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>2631</td>\n",
       "      <td>SCOTT GERWIG</td>\n",
       "      <td>M</td>\n",
       "      <td>PORTLAND</td>\n",
       "      <td>OR</td>\n",
       "      <td>19:57</td>\n",
       "      <td>6:26</td>\n",
       "      <td>19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>2431</td>\n",
       "      <td>NICOLAUS L ROCK</td>\n",
       "      <td>M</td>\n",
       "      <td>HILLSBORO</td>\n",
       "      <td>OR</td>\n",
       "      <td>20:00</td>\n",
       "      <td>6:27</td>\n",
       "      <td>20:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Place    Bib               Name  Gender        City  State  Chip Time  \\\n",
       "5      1   2320  DANIEL M HINCKLEY       M   HILLSBORO     OR      16:42   \n",
       "6      2   2335        KORY F GRAY       M   HILLSBORO     OR      17:34   \n",
       "7      3   1770      FILIP SCHMOLE       M    PORTLAND     OR      18:13   \n",
       "8      4   2584  TRENTON C ROLLING       M    PORTLAND     OR      18:32   \n",
       "9      5   2688       YEAN-AN LIAO       M   HILLSBORO     OR      19:12   \n",
       "10     6   1576       JORGE1 LOPEZ       M    PORTLAND     OR      19:19   \n",
       "11     7   1479  SCOTT E HAMPSHIRE       M   HILLSBORO     OR      19:27   \n",
       "12     8    895       KEVIN CANADA       M   BEAVERTON     OR      19:53   \n",
       "13     9   2631       SCOTT GERWIG       M    PORTLAND     OR      19:57   \n",
       "14    10   2431    NICOLAUS L ROCK       M   HILLSBORO     OR      20:00   \n",
       "\n",
       "    Chip Pace  Gun Time  \n",
       "5        5:23     16:44  \n",
       "6        5:40     17:35  \n",
       "7        5:52     18:14  \n",
       "8        5:58     18:35  \n",
       "9        6:11     19:18  \n",
       "10       6:14     19:20  \n",
       "11       6:16     19:29  \n",
       "12       6:24     20:02  \n",
       "13       6:26     19:59  \n",
       "14       6:27     20:01  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We remove uneccessary characters from the header\n",
    "df_header2[0] = df_header2[0].str.strip('[')\n",
    "df_header2[8] = df_header2[8].str.strip(']')\n",
    "\n",
    "# We concatenate the two dataframes\n",
    "frames = [df_header2, df_table3]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df2 = df.rename(columns=df.iloc[0]) # We assign the first row to be the dataframe header\n",
    "df3 = df2.drop(df2.index[0]) # We drop the replicated header from the first row of the dataframe\n",
    "\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! It took a while to get here, but at this point, the dataframe is in the desired format. Now the table has been both scraped from the web and has been converted into an appropriate representation where we can apply data mining operations covered through the previous lectures and labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Second example - scraping URLs\n",
    "\n",
    "For this second example, we will see how to scrape information from a fictional store, in this case a book store which is available at the following URL: [http://books.toscrape.com/](http://books.toscrape.com/)\n",
    "\n",
    "Please visit the above website, inspect the webpage, and inspect the HTML source code from your browser (using the same process described in section 2 of the lab notebook).\n",
    "\n",
    "We see that the webpage lists 20 books. For each book, there is an associated URL, which points to a separate webpage describing each book in detail. The goal of this example is to scrape the URLs for all these 20 books.\n",
    "\n",
    "We first follow the same process as in sections 1 and 2 of the lab notebook to open the URL and parse the HTML content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bookstore = \"http://books.toscrape.com/index.html\"\n",
    "html_bookstore = urlopen(url_bookstore)\n",
    "soup_bookstore = BeautifulSoup(html_bookstore, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the HTML code of the [webpage](http://books.toscrape.com/), we see that each of the 20 books is mentioned under an 'article' tag with the value 'product_pod'. Under each mention of the 'product_pod' value, there is the corresponding URL for each book, under the 'a' tag, and specifically the 'href' attribute. This seems to be a reliable source to spot product URLs.\n",
    "\n",
    "So the first thing to attempt is to find 'article' tags in the HTML code that contain the 'product_pod' value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<article class=\"product_pod\">\n",
       "<div class=\"image_container\">\n",
       "<a href=\"catalogue/a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>\n",
       "</div>\n",
       "<p class=\"star-rating Three\">\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "</p>\n",
       "<h3><a href=\"catalogue/a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\n",
       "<div class=\"product_price\">\n",
       "<p class=\"price_color\">£51.77</p>\n",
       "<p class=\"instock availability\">\n",
       "<i class=\"icon-ok\"></i>\n",
       "    \n",
       "        In stock\n",
       "    \n",
       "</p>\n",
       "<form>\n",
       "<button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">Add to basket</button>\n",
       "</form>\n",
       "</div>\n",
       "</article>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to produce too much information. So instead of only looking for the 'product_pod' value in an 'article' tag, let's look for URLs only. \n",
    "\n",
    "If we inspect the HTML code further, we see that the URLs we are looking for are within the 'a' tag, which is within the 'div' tag. Soe we can modify the above command as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"catalogue/a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\").div.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is beter, we now have all information contained within the 'a' tag for a book. But we only need the URL contained in the 'href' value, which in the above example should be \"catalogue/a-light-in-the-attic_1000/index.html\".\n",
    "\n",
    "We can get this URL by adding .get('href') to the previous instruction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catalogue/a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now managed to get our first product URL with BeautifulSoup. Now let’s gather all the product URLs on the main web page at once using the findAll() function, which iterates across all mentions of the 'product_pod' value within an 'article' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 fetched book URLs\n",
      "catalogue/a-light-in-the-attic_1000/index.html\n",
      "catalogue/tipping-the-velvet_999/index.html\n",
      "catalogue/soumission_998/index.html\n",
      "catalogue/sharp-objects_997/index.html\n",
      "catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n",
      "catalogue/the-requiem-red_995/index.html\n",
      "catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "catalogue/the-black-maria_991/index.html\n",
      "catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "catalogue/shakespeares-sonnets_989/index.html\n",
      "catalogue/set-me-free_988/index.html\n",
      "catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "catalogue/rip-it-up-and-start-again_986/index.html\n",
      "catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "catalogue/olio_984/index.html\n",
      "catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "catalogue/libertarianism-for-beginners_982/index.html\n",
      "catalogue/its-only-the-himalayas_981/index.html\n"
     ]
    }
   ],
   "source": [
    "book_urls = [x.div.a.get('href') for x in soup_bookstore.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "# Display number of fetched URLs\n",
    "print(str(len(book_urls)) + \" fetched book URLs\")\n",
    "\n",
    "# We can print all fetched URLS\n",
    "for book in book_urls:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now managed to fetch all 20 URLs corresponding to each book in the website, and placed them in a list object in python, which can be used for further data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Assignment</font>\n",
    "\n",
    "The first two questions are coding exercises; question 3 does not require you to write any code.\n",
    "\n",
    "The supplementary material for this week contains offline copies of the assignment material in case there are internet connectivity issues with School websites. The supplementary material includes file \"income_table.html\" (for Question 1), file \"programmes.html\" (for Question 2), and file \"Question-3-Graph.png\" (for Question 3).\n",
    "\n",
    "1. You are provided with the following URL: [http://eecs.qmul.ac.uk/~emmanouilb/income_table.html](http://eecs.qmul.ac.uk/~emmanouilb/income_table.html). This webpage includes a table on individuals' income and shopping habits - the same that was used in the Week 3 lab.\n",
    "  1. Inspect the HTML code of the above URL, and provide a short report on the various tags present in the code. What is the function of each unique tag present in the HTML code? [0.5 marks out of 5]\n",
    "  2. Using Beautiful Soup, scrape the table and convert it into a pandas dataframe. Perform data cleaning when necessary to remove extra characters (no need to handle missing values). In the report include the code that was used to scrape and convert the table and provide evidence that the table has been successfully scraped and converted (e.g. by displaying the contents of the dataframe). [1 mark out of 5]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open the URL\n",
    "url = \"http://eecs.qmul.ac.uk/~emmanouilb/income_table.html\"\n",
    "html = urlopen(url)\n",
    "\n",
    "# Get BS4 ready:\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "#print(type(soup))\n",
    "\n",
    "# Present the various tags present in the code: - This gives all.\n",
    "#soup.find_all(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Region</td>\n",
       "      <td>Age</td>\n",
       "      <td>Income</td>\n",
       "      <td>Online Shopper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1       2               3\n",
       "0  Region  Age  Income  Online Shopper"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Online Shopper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>49</td>\n",
       "      <td>86400</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>32</td>\n",
       "      <td>57600</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA</td>\n",
       "      <td>35</td>\n",
       "      <td>64800</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>43</td>\n",
       "      <td>73200</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USA</td>\n",
       "      <td>45</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India</td>\n",
       "      <td>40</td>\n",
       "      <td>69600</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brazil</td>\n",
       "      <td></td>\n",
       "      <td>62400</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>India</td>\n",
       "      <td>53</td>\n",
       "      <td>94800</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USA</td>\n",
       "      <td>55</td>\n",
       "      <td>99600</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>India</td>\n",
       "      <td>42</td>\n",
       "      <td>80400</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Region Age Income Online Shopper\n",
       "1    India  49  86400             No\n",
       "2   Brazil  32  57600            Yes\n",
       "3      USA  35  64800             No\n",
       "4   Brazil  43  73200             No\n",
       "5      USA  45                   Yes\n",
       "6    India  40  69600            Yes\n",
       "7   Brazil      62400             No\n",
       "8    India  53  94800            Yes\n",
       "9      USA  55  99600             No\n",
       "10   India  42  80400            Yes"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape and Clean the table - Form a dataframe\n",
    "\n",
    "# Header Preperation:\n",
    "header_list = []\n",
    "\n",
    "column_labels = str(soup.find_all('th'))\n",
    "cleantext_header = BeautifulSoup(column_labels, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "header_list.append(cleantext_header) # Add the clean table header to the list\n",
    "# print(header_list)\n",
    "\n",
    "df_header = pd.DataFrame(header_list)\n",
    "df_header2 = df_header[0].str.split(', ', expand=True)\n",
    "df_header2[0] = df_header2[0].str.strip('[')\n",
    "df_header2[3] = df_header2[3].str.strip(']')\n",
    "display(df_header2.head())\n",
    "\n",
    "# Table Preperation:\n",
    "table_list = []\n",
    "\n",
    "# Print the first 10 table rows\n",
    "rows = soup.find_all('tr')  # the 'tr' tag in html denotes a table row\n",
    "#print(rows[:10])\n",
    "\n",
    "# For every row in the table, find each cell element and add it to the list\n",
    "for row in rows:\n",
    "    row_td_cells = str(row.find_all('td'))\n",
    "    row_cleantext = BeautifulSoup(row_td_cells, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "    table_list.append(row_cleantext)  # Add the clean table row to the list \n",
    "#print(table_list)\n",
    "\n",
    "df_table = pd.DataFrame(table_list)\n",
    "df_table2 = df_table[0].str.split(', ', expand=True)\n",
    "df_table2.head(10)\n",
    "\n",
    "# Remove uneccesary characters\n",
    "df_table2[0] = df_table2[0].str.strip('[')\n",
    "df_table2[3] = df_table2[3].str.strip(']')\n",
    "\n",
    "# Place everything in:\n",
    "frames = [df_header2, df_table2]\n",
    "df = pd.concat(frames)\n",
    "df2 = df.rename(columns=df.iloc[0]) # We assign the first row to be the dataframe header\n",
    "df3 = df2.drop(df2.index[0]) # We drop the replicated header from the first row of the dataframe\n",
    "df3.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The list of the various MSc programmes offered by the School of EECS is provided at the following URL: [http://eecs.qmul.ac.uk/postgraduate/programmes/](http://eecs.qmul.ac.uk/postgraduate/programmes/). Perform web scraping on the table present in the above URL and convert it into a pandas dataframe that would include one row for each programme of study as shown in the webpage. The dataframe should include the following 5 columns: name of postgraduate degree programme (e.g. Advanced Electronic and Electrical Engineering), programme code for part-time study (e.g. H60C), programme code for full-time study (e.g. H60A), URL for part-time study programme details, URL for full-time study programme details. Perform data cleaning to remove unecessary characters when needed. In the report include the code that was used to scrape, convert and clean the table and provide evidence that the table has been successfully scraped (e.g. by displaying the contents of the dataframe). [1 mark out of 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open the URL\n",
    "url = \"http://eecs.qmul.ac.uk/postgraduate/programmes/\"\n",
    "html = urlopen(url)\n",
    "\n",
    "# Get BS4 ready:\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "#print(type(soup))\n",
    "\n",
    "# Inspect HTML for finding a way to scrape this:\n",
    "# soup.find_all(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Postgraduate degree programmes</td>\n",
       "      <td>Part-time(2 year)</td>\n",
       "      <td>Full-time(1 year)</td>\n",
       "      <td>Part-time URL(2 year)</td>\n",
       "      <td>Full-time URL(1 year)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0                  1                  2  \\\n",
       "0  Postgraduate degree programmes  Part-time(2 year)  Full-time(1 year)   \n",
       "\n",
       "                       3                      4  \n",
       "0  Part-time URL(2 year)  Full-time URL(1 year)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Header Preperation:\n",
    "header_list = []\n",
    "\n",
    "column_labels = str(soup.find_all('th'))\n",
    "cleantext_header = BeautifulSoup(column_labels, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "header_list.append(cleantext_header) # Add the clean table header to the list\n",
    "#print(header_list)\n",
    "\n",
    "df_header = pd.DataFrame(header_list)\n",
    "df_header2 = df_header[0].str.split(', ', expand=True)\n",
    "df_header2[0] = df_header2[0].str.strip('[')\n",
    "df_header2[2] = df_header2[2].str.strip(']')\n",
    "df_header2[3] = ['Part-time URL(2 year)']\n",
    "df_header2[4] = ['Full-time URL(1 year)']\n",
    "display(df_header2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postgraduate degree programmes</th>\n",
       "      <th>Part-time(2 year)</th>\n",
       "      <th>Full-time(1 year)</th>\n",
       "      <th>Part-time URL(2 year)</th>\n",
       "      <th>Full-time URL(1 year)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>I4U2</td>\n",
       "      <td>I4U1</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big Data Science</td>\n",
       "      <td>H6J6</td>\n",
       "      <td>H6J7</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computer Games</td>\n",
       "      <td></td>\n",
       "      <td>I4U4</td>\n",
       "      <td></td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computer Science</td>\n",
       "      <td>G4U2</td>\n",
       "      <td>G4U1</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Computer Science by Research</td>\n",
       "      <td>G4Q2</td>\n",
       "      <td>G4Q1</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Computing and Information Systems</td>\n",
       "      <td>G5U6</td>\n",
       "      <td>G5U5</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Science and Artificial Intelligence by Co...</td>\n",
       "      <td></td>\n",
       "      <td>I4U5</td>\n",
       "      <td></td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Electronic Engineering by Research</td>\n",
       "      <td>H6T6</td>\n",
       "      <td>H6T5</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Internet of Things (Data)</td>\n",
       "      <td>I1T2</td>\n",
       "      <td>I1T0</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machine Learning for Visual Data Analytics</td>\n",
       "      <td>H6JZ</td>\n",
       "      <td>H6JE</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Media and Arts Technology by Research</td>\n",
       "      <td></td>\n",
       "      <td>G4Q3</td>\n",
       "      <td></td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sound and Music Computing</td>\n",
       "      <td>H6T4</td>\n",
       "      <td>H6T8</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Telecommunication and Wireless Systems</td>\n",
       "      <td>H6JD</td>\n",
       "      <td>H6JA</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Digital and Technology Solutions (Apprenticeship)</td>\n",
       "      <td>I4DA</td>\n",
       "      <td></td>\n",
       "      <td>https://www.qmul.ac.uk/postgraduate/taught/cou...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Postgraduate degree programmes Part-time(2 year)  \\\n",
       "1                             Artificial Intelligence             I4U2    \n",
       "2                                    Big Data Science              H6J6   \n",
       "3                                      Computer Games                     \n",
       "4                                    Computer Science              G4U2   \n",
       "5                        Computer Science by Research              G4Q2   \n",
       "6                   Computing and Information Systems              G5U6   \n",
       "7   Data Science and Artificial Intelligence by Co...                     \n",
       "8                  Electronic Engineering by Research              H6T6   \n",
       "9                           Internet of Things (Data)              I1T2   \n",
       "10         Machine Learning for Visual Data Analytics              H6JZ   \n",
       "11              Media and Arts Technology by Research                     \n",
       "12                         Sound and Music Computing               H6T4   \n",
       "13             Telecommunication and Wireless Systems              H6JD   \n",
       "14  Digital and Technology Solutions (Apprenticeship)              I4DA   \n",
       "\n",
       "   Full-time(1 year)                              Part-time URL(2 year)  \\\n",
       "1              I4U1   https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "2               H6J7  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "3               I4U4                                                      \n",
       "4               G4U1  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "5               G4Q1  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "6               G5U5  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "7              I4U5                                                       \n",
       "8               H6T5  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "9               I1T0  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "10              H6JE  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "11              G4Q3                                                      \n",
       "12              H6T8  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "13              H6JA  https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "14                    https://www.qmul.ac.uk/postgraduate/taught/cou...   \n",
       "\n",
       "                                Full-time URL(1 year)  \n",
       "1   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "2   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "3   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "4   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "5   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "6   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "7   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "8   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "9   https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "10  https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "11  https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "12  https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "13  https://www.qmul.ac.uk/postgraduate/taught/cou...  \n",
       "14                                                     "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all rows & print a sample row:\n",
    "rows = soup.find_all('tr')  # the 'tr' tag in html denotes a table row\n",
    "# print(rows[3])\n",
    "# print()\n",
    "\n",
    "table_list = []\n",
    "for row in rows[1:]: # ignore header\n",
    "    # Get the texts:\n",
    "    columns = row.find_all('td')  # the 'td' tag in html code denotes a table cell  \n",
    "    cols_str = str(columns)\n",
    "    \n",
    "    cleantext = BeautifulSoup(cols_str, \"lxml\").get_text()\n",
    "#     print(cleantext)\n",
    "\n",
    "    \n",
    "    # Get the links:\n",
    "    for column in columns[1:]:\n",
    "        if(column.find('a') != None):\n",
    "            links = column.find('a').get('href')\n",
    "            cleantext = cleantext + ', ' + links\n",
    "#             print(links)\n",
    "        else:\n",
    "            links = None\n",
    "            cleantext = cleantext + ', ' + ''\n",
    "            \n",
    "    table_list.append(cleantext)\n",
    "    \n",
    "# Construct the structure:\n",
    "df_table = pd.DataFrame(table_list)\n",
    "df_table2 = df_table[0].str.split(', ', expand=True)\n",
    "\n",
    "# Cleaning: - Remove Uncessary Characters:\n",
    "df_table2[0] = df_table2[0].str.strip('[')\n",
    "df_table2[2] = df_table2[2].str.strip(']')\n",
    "\n",
    "# df_table2.head(20)\n",
    "\n",
    "# Tie the header and the list:\n",
    "frames = [df_header2, df_table2]\n",
    "df = pd.concat(frames)\n",
    "df2 = df.rename(columns=df.iloc[0]) # We assign the first row to be the dataframe header\n",
    "df3 = df2.drop(df2.index[0]) # We drop the replicated header from the first row of the dataframe\n",
    "df3.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider the graph in the figure below as displaying the links for a group of 5 webpages.\n",
    "  1. Which of the 5 nodes would you consider hubs and which would you consider authorities? Explain why. [0.5 marks out of 5]\n",
    "  2. Assume that this graph is to be used as input to the PageRank algorithm. Calculate the transition probabilities $p_{ij}$ for all 5 nodes in the below graph (where $i$ and $j$ take values between 1 to 5). Add transitions with a uniform probability distribution in the case of dead-end nodes (do not consider cases of dead-end components). [1 mark out of 5].\n",
    "  3. Derive the PageRank $\\pi(i)$ for all nodes, where $i=\\{1,...,5\\}$ corresponds to the node index. Assume that the teleportation probability is set to $\\alpha$. [1 mark out of 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FigGraph](http://eecs.qmul.ac.uk/~emmanouilb/FigGraph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
